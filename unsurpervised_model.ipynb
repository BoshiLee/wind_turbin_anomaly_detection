{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2903bf40e913349b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 指定 STFT 參數\n",
    "load_dotenv()\n",
    "sample_rate = int(os.getenv('sample_rate'))\n",
    "n_mels = int(os.getenv('n_mels'))\n",
    "# n_fft = int(os.getenv('n_fft'))\n",
    "\n",
    "print(f'Sample rate: {sample_rate}, n_mels: {n_mels}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_segmented_files(directory):\n",
    "    wav_files = []\n",
    "    for dir in os.listdir(directory):\n",
    "        if dir.endswith('_anomaly'):\n",
    "            continue\n",
    "        for file in os.listdir(os.path.join(directory, dir)):\n",
    "            if not file.endswith(\".wav\"):\n",
    "                continue\n",
    "            file_path = os.path.join(directory, dir, file)\n",
    "            y, sr = librosa.load(file_path, sr=None)\n",
    "            filename = f'{dir}_{file}'\n",
    "            wav_files.append((y, filename))\n",
    "\n",
    "    return wav_files"
   ],
   "id": "b8b19fb5b9994532",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 讀取分割後的音訊片段\n",
    "normal_segments = load_segmented_files('audio')"
   ],
   "id": "f5f962a3285fe0d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from conver_mel_spectrogram import compute_mel_spectrogram\n",
    "def convert_to_mel_spectrogram(audio, sr):\n",
    "    mel_spectrogram, hop_length = compute_mel_spectrogram(audio, sr)\n",
    "    return mel_spectrogram"
   ],
   "id": "52bfe354760bdac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 打亂 mel_spectrograms_normal 列表\n",
    "np.random.shuffle(normal_segments)\n",
    "print(f'Number of normal segments: {len(normal_segments)}')\n",
    "audio_data, filename = normal_segments[0]\n",
    "print(f'Audio data shape: {audio_data.shape}, filename: {filename}')\n",
    "mel_spectrogram_db, hop_length = compute_mel_spectrogram(audio_data, sample_rate)\n",
    "print(f\"Mel spectrogram shape: {mel_spectrogram_db.shape}\")\n",
    "print(mel_spectrogram_db, hop_length)\n"
   ],
   "id": "a4572e0e9dc6ab1b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "mel_spectrograms_normal = [convert_to_mel_spectrogram(audio[0], sample_rate) for audio in tqdm(normal_segments)]",
   "id": "a93915a4e5e5f6a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from conver_mel_spectrogram import plot_mel_spectrogram\n",
    "plot_mel_spectrogram(mel_spectrograms_normal[10], hop_length=hop_length, sample_rate=sample_rate, save_file=False, camp='jet') \n",
    "print(f\"Mel spectrogram: filename: {normal_segments[10][1]}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c013c9f2c385c9f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Spilt Dataset to training and testing"
   ],
   "id": "25cd908d98a04189"
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 將梅爾頻譜圖轉換為numpy數組\n",
    "mel_spectrograms_array = np.array(mel_spectrograms_normal)\n",
    "\n",
    "# 對數據進行切分\n",
    "train_data, test_data = train_test_split(mel_spectrograms_array, test_size=0.2, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"訓練集數據形狀:\", train_data.shape)\n",
    "print(\"驗證集數據形狀:\", val_data.shape)\n",
    "print(\"測試集數據形狀:\", test_data.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79cf0c41705e2324",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 調整數據的形狀"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c189551fa2d8e31"
  },
  {
   "cell_type": "code",
   "source": [
    "# 調整數據的形狀\n",
    "train_data = train_data.reshape(train_data.shape[0], train_data.shape[1], train_data.shape[2], 1)\n",
    "val_data = val_data.reshape(val_data.shape[0], val_data.shape[1], val_data.shape[2], 1)\n",
    "test_data = test_data.reshape(test_data.shape[0], test_data.shape[1], test_data.shape[2], 1)\n",
    "\n",
    "# 數據歸一化到 [0, 1]\n",
    "train_data = (train_data - train_data.min()) / (train_data.max() - train_data.min())\n",
    "val_data = (val_data - val_data.min()) / (val_data.max() - val_data.min())\n",
    "test_data = (test_data - test_data.min()) / (test_data.max() - test_data.min())\n",
    "# 打印轉換後的數據形狀\n",
    "print(\"訓練集數據形狀:\", train_data.shape)\n",
    "print(\"驗證集數據形狀:\", val_data.shape)\n",
    "print(\"測試集數據形狀:\", test_data.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad3e1cec51e54d60",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 創建 CNN Autoencoder 模型"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b02d3a0bff17c8b5"
  },
  {
   "cell_type": "code",
   "source": [
    "from keras.layers import Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Dense, Flatten, Reshape, \\\n",
    "    BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "\n",
    "def create_cnn_autoencoder(input_shape):\n",
    "    input_img = Input(shape=input_shape)\n",
    "    input_height = input_shape[0]\n",
    "    input_width = input_shape[1]\n",
    "    \n",
    "    # Encoder\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.001))(input_img)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.001))(x)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.001))(x)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.001))(x)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = Conv2D(16, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.001))(x)\n",
    "    encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    \n",
    "    # Flatten and Dense layers at bottleneck\n",
    "    x = Flatten()(encoded)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(encoded.shape[1] * encoded.shape[2] * encoded.shape[3], activation='relu')(x)\n",
    "    x = Reshape((encoded.shape[1], encoded.shape[2], encoded.shape[3]))(x)\n",
    "    \n",
    "    # Decoder\n",
    "    x = Conv2D(16, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.001))(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.001))(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.001))(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.001))(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.001))(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    \n",
    "    # Ensure the output has the same size as input\n",
    "    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "    decoded = Lambda(lambda x: x[:, :input_height, :input_width, :])(decoded)\n",
    "    \n",
    "    autoencoder = Model(input_img, decoded)\n",
    "    return autoencoder\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e1ac3c3213546fb8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 初始化模型"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ef2b95e55ea6d98"
  },
  {
   "cell_type": "code",
   "source": [
    "# 指定輸入形狀\n",
    "input_shape = np.shape(train_data)[1:]\n",
    "\n",
    "# 創建 CNN Autoencoder 模型\n",
    "autoencoder = create_cnn_autoencoder(input_shape)\n",
    "autoencoder.summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84d8bdabcfeac3e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def plot_model(autoencoder):\n",
    "    from tensorflow.keras.utils import plot_model\n",
    "    plot_model(autoencoder, to_file='autoencoder.png', show_shapes=True)\n",
    "    \n",
    "plot_model(autoencoder)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40de000e36ded967",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c3dee148eea3c59f"
  },
  {
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.optimizers import Adam \n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "import tensorflow as tf\n",
    "# 設定初始學習率\n",
    "initial_learning_rate = 0.0001\n",
    "\n",
    "# 創建 Adam 優化器並指定學習率\n",
    "optimizer = Adam(learning_rate=initial_learning_rate)\n",
    "\n",
    "def weighted_loss_function(y_true, y_pred):\n",
    "    weights = np.ones_like(y_true)\n",
    "    weights[:, :weights.shape[1] // 2] = 1.5  # 高頻段給予更高的權重\n",
    "    return np.sum(weights * (y_true * np.log(y_pred + 1e-7) + (1 - y_true) * np.log(1 - y_pred + 1e-7)))\n",
    "\n",
    "\n",
    "# 編譯模型\n",
    "autoencoder.compile(optimizer=optimizer, loss=weighted_loss_function)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e23048612884a57",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 訓練模型"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7d6f86bbefd95c2"
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, mode='min', restore_best_weights=True)\n",
    "# 定義學習率調整策略\n",
    "history = autoencoder.fit(train_data, train_data,\n",
    "                epochs=200,\n",
    "                batch_size=16,\n",
    "                shuffle=True,\n",
    "                validation_data=(test_data, test_data),\n",
    "                callbacks=[early_stopping],\n",
    "                verbose=2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce8aec599434993b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 評估模型"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "85394d3d9a07b723"
  },
  {
   "cell_type": "code",
   "source": [
    "test_loss = autoencoder.evaluate(val_data, val_data)\n",
    "print(\"Test loss:\", test_loss)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a218a1aa00ee2711",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 繪製訓練過程"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1cdf051f45005f0d"
  },
  {
   "cell_type": "code",
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper right')\n",
    "plt.savefig('model_loss.png')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "403e4c3f84439f45",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 儲存模型"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eac6b2b2558d7748"
  },
  {
   "cell_type": "code",
   "source": "autoencoder.save('autoencoder_10_11.h5')",
   "metadata": {
    "collapsed": false
   },
   "id": "1e2c2c973d198acb",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 使用測試集數據進行預測"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0fee0a3d0ab9c3b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def calculate_reconstruction_error(original, reconstructed):\n",
    "    # 如果只有單個樣本，確保數據是 4D\n",
    "    if len(original.shape) == 3:\n",
    "        original = np.expand_dims(original, axis=0)\n",
    "        reconstructed = np.expand_dims(reconstructed, axis=0)\n",
    "    return np.mean(np.square(original - reconstructed), axis=(1, 2, 3))"
   ],
   "id": "897233fb75800991",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "source": [
    "# 計算每個樣本的重建損失\n",
    "reconstructed_data = autoencoder.predict(train_data)\n",
    "reconstruction_errors = calculate_reconstruction_error(train_data, reconstructed_data)\n",
    "\n",
    "# 設定一個閾值來判斷異常\n",
    "threshold = np.mean(reconstruction_errors) + 3 * np.std(reconstruction_errors)\n",
    "print(f'Threshold: {threshold}')"
   ],
   "id": "5fdd701755242fa4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 測試數據集上的重建\n",
    "reconstructed_test_data = autoencoder.predict(test_data)\n",
    "\n",
    "# 計算測試數據集上的重建誤差\n",
    "test_reconstruction_errors = calculate_reconstruction_error(test_data, reconstructed_test_data)\n",
    "\n",
    "# 判斷異常樣本\n",
    "anomalies = test_reconstruction_errors > threshold\n",
    "normal = test_reconstruction_errors <= threshold\n",
    "print(f\"Number of anomalies detected: {np.sum(anomalies)}\")\n",
    "\n",
    "# 可視化一些異常樣本\n",
    "anomalous_data = test_data[anomalies]\n",
    "anomalous_reconstructed_data = reconstructed_test_data[anomalies]"
   ],
   "id": "a93c8335a56ab842",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 測試單個樣本的重建誤差\n",
    "error = calculate_reconstruction_error(test_data[0], reconstructed_test_data[0])[0]"
   ],
   "id": "7a4568fc124bec0e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def plot_comparison(input_mel_spectrogram, output_mel_spectrogram, input_title, output_title, threshold, loss, output_dir='images/reconstruct_anomaly', output_name='comparison', save_only=False):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    \n",
    "    # 繪製輸入梅爾頻譜圖\n",
    "    plt.subplot(1, 2, 1)\n",
    "    librosa.display.specshow(input_mel_spectrogram, x_axis='time', y_axis='mel', sr=sample_rate, hop_length=hop_length, cmap='jet')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(input_title)\n",
    "    \n",
    "    # 繪製輸出梅爾頻譜圖\n",
    "    plt.subplot(1, 2, 2)\n",
    "    librosa.display.specshow(output_mel_spectrogram, x_axis='time', y_axis='mel', sr=sample_rate, hop_length=hop_length, cmap='jet')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(output_title)\n",
    "    # plot mse if mse in red if mse > mse_threshold\n",
    "    plt.text(0, 0, f'loss: {loss}', fontsize=12, color='red' if loss > threshold else 'black', backgroundcolor='white')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_only:\n",
    "        plt.savefig(f'{output_dir}/{output_name}.png')\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f0aa6345d440867",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "anomalies_index = np.where(anomalies)[0]\n",
    "for i in tqdm(anomalies_index):\n",
    "    plot_comparison(test_data[i].reshape(test_data[i].shape[0], test_data[i].shape[1]),\n",
    "                    reconstructed_test_data[i].reshape(reconstructed_test_data[i].shape[0], reconstructed_test_data[i].shape[1]),\n",
    "                    'Original Mel Spectrogram', 'Reconstructed Mel Spectrogram', \n",
    "                    threshold=threshold, loss=test_reconstruction_errors[i],\n",
    "                    output_dir='images/reconstruct_anomaly',\n",
    "                    output_name=f'anomaly_{i}', save_only=True)"
   ],
   "id": "b634e8adc128a27c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "normal_index = np.where(normal)[0]\n",
    "for i in tqdm(normal_index):\n",
    "    loss = calculate_reconstruction_error(test_data[i], reconstructed_test_data[i])\n",
    "    plot_comparison(test_data[i].reshape(test_data[i].shape[0], test_data[i].shape[1]),\n",
    "                    reconstructed_test_data[i].reshape(reconstructed_test_data[i].shape[0], reconstructed_test_data[i].shape[1]),\n",
    "                    'Original Mel Spectrogram', 'Reconstructed Mel Spectrogram', threshold=threshold, loss=loss, \n",
    "                    output_dir='images/reconstruct_normal',\n",
    "                    output_name=f'normal_{i}', save_only=True)"
   ],
   "id": "41d22cc79aa130f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "計算重建失敗的數量"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f08f171f287e88be"
  },
  {
   "cell_type": "code",
   "source": [
    "print(f'Failed reconstruction count is {len(anomalies_index)}, total test data is {len(test_data)}')\n",
    "print(f'Failed reconstruction rate is {len(anomalies_index) / len(test_data) * 100}%')\n",
    "print(f'Success reconstruction rate is {100 - len(anomalies_index) / len(test_data) * 100}%')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a320712521e2e4bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "92257b43d53843a6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
